{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.1.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (1.21.2)\n"
     ]
    }
   ],
   "source": [
    "import re  \n",
    "import pandas as pd \n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "!pip install gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download and tokenize\n",
    "- 'simpson.csv' 다운로드 & 토크나이징\n",
    "\n",
    "### Make a Word2Vec model\n",
    "- Parameters\n",
    "- 1) min_count=30\n",
    "- 2) window=10\n",
    "- 3) workers=1\n",
    "\n",
    "### Find Most similar words (most_similar 사용)\n",
    "- 'Homer'와 가장 유사한 단어 20개 출력 \n",
    "- 'Bart'와 가장 유사한 단어 20개 출력\n",
    "\n",
    "### Calculate Similarity (similarity 사용)\n",
    "- 'Maggie'와 'baby'의 similarity 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data download and tokenize\n",
    "* Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131848</th>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131849</th>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131850</th>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131851</th>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131852</th>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>131853 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Caption\n",
       "0       No, actually, it was a little of both. Sometim...\n",
       "1                                  Where's Mr. Bergstrom?\n",
       "2       I don't know. Although I'd sure like to talk t...\n",
       "3                              That life is worth living.\n",
       "4       The polls will be open from now until the end ...\n",
       "...                                                   ...\n",
       "131848                                          I'm back.\n",
       "131849  You see, class, my Lyme disease turned out to ...\n",
       "131850                                 Psy-cho-so-ma-tic.\n",
       "131851                     Does that mean you were crazy?\n",
       "131852                  No, that means she was faking it.\n",
       "\n",
       "[131853 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pd.read_csv('simpson.csv')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 cleaning\n",
    "def clean_text(text):\n",
    "    \n",
    "    pattern = '(\\[a-zA-Z0-9\\_.+-\\]+@\\[a-zA-Z0-9]+.\\[a-zA-Z0-9-.\\]+)' # email제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "    \n",
    "    pattern = re.compile(r'([^\\w]?\\d+\\.?\\,?\\)?\\d*)+') # 숫자 제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "    \n",
    "    pattern = '<[^>]*>' # html 태그 제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "    \n",
    "    pattern = '[\\r|\\n]' # \\r,\\n 제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "    \n",
    "    pattern =  '[^\\w\\s]' # 특수기호 제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "    \n",
    "    pattern = re.compile(r'\\s+')  #  이중 space 제거\n",
    "    text = re.sub(pattern = pattern,repl = ' ',string = text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         No actually it was a little of both Sometimes ...\n",
       "1                                     Where s Mr Bergstrom \n",
       "2         I don t know Although I d sure like to talk to...\n",
       "3                                That life is worth living \n",
       "4         The polls will be open from now until the end ...\n",
       "                                ...                        \n",
       "131848                                            I m back \n",
       "131849      You see class my Lyme disease turned out to be \n",
       "131850                                   Psy cho so ma tic \n",
       "131851                       Does that mean you were crazy \n",
       "131852                     No that means she was faking it \n",
       "Name: Caption, Length: 131853, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = doc['Caption'].apply(clean_text)\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [No, actually, it, was, a, little, of, both, S...\n",
       "1                                 [Where, s, Mr, Bergstrom]\n",
       "2         [I, don, t, know, Although, I, d, sure, like, ...\n",
       "3                           [That, life, is, worth, living]\n",
       "4         [The, polls, will, be, open, from, now, until,...\n",
       "                                ...                        \n",
       "131848                                         [I, m, back]\n",
       "131849    [You, see, class, my, Lyme, disease, turned, o...\n",
       "131850                              [Psy, cho, so, ma, tic]\n",
       "131851                 [Does, that, mean, you, were, crazy]\n",
       "131852              [No, that, means, she, was, faking, it]\n",
       "Name: Caption, Length: 131853, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize_result = doc.apply(word_tokenize)\n",
    "word_tokenize_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Word2Vec model\n",
    "- Parameters\n",
    "- 1) min_count=30\n",
    "- 2) window=10\n",
    "- 3) workers=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 명령어 실행하시고\n",
    "def hash32(value):\n",
    "     return hash(value) & 0xffffffff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total = gensim.models.Word2Vec(word_tokenize_result, window = 10, min_count = 30, workers = 1, hashfxn=hash32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most similar words (most_similar 사용)\n",
    "- 'Homer'와 가장 유사한 단어 20개 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Marge', 0.7572533488273621),\n",
       " ('Bart', 0.7568457126617432),\n",
       " ('Mr', 0.6498435735702515),\n",
       " ('Lisa', 0.6484478712081909),\n",
       " ('Mrs', 0.6059045195579529),\n",
       " ('Grampa', 0.537876546382904),\n",
       " ('Abe', 0.527523398399353),\n",
       " ('Ned', 0.5205222964286804),\n",
       " ('Dad', 0.5137090086936951),\n",
       " ('Monty', 0.4828593134880066),\n",
       " ('Milhouse', 0.44829511642456055),\n",
       " ('son', 0.4297706186771393),\n",
       " ('family', 0.4273833930492401),\n",
       " ('Moe', 0.42662709951400757),\n",
       " ('Homie', 0.41260239481925964),\n",
       " ('boy', 0.40894970297813416),\n",
       " ('Abraham', 0.3914429843425751),\n",
       " ('Mister', 0.3870508372783661),\n",
       " ('Chief', 0.38313689827919006),\n",
       " ('Maggie', 0.38289517164230347)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_total.wv.most_similar('Homer', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'Bart'와 가장 유사한 단어 20개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Lisa', 0.8259381055831909),\n",
       " ('Homer', 0.7568456530570984),\n",
       " ('Dad', 0.6413039565086365),\n",
       " ('Grampa', 0.6366949081420898),\n",
       " ('Milhouse', 0.6109604239463806),\n",
       " ('Marge', 0.6091436147689819),\n",
       " ('Maggie', 0.5209029316902161),\n",
       " ('son', 0.5169365406036377),\n",
       " ('Mom', 0.48688992857933044),\n",
       " ('boy', 0.4771064221858978),\n",
       " ('Mrs', 0.47028690576553345),\n",
       " ('father', 0.45897993445396423),\n",
       " ('Daddy', 0.43136802315711975),\n",
       " ('mother', 0.4307199716567993),\n",
       " ('Mr', 0.4267365038394928),\n",
       " ('Abe', 0.4061337411403656),\n",
       " ('Ned', 0.40542250871658325),\n",
       " ('you', 0.4014216661453247),\n",
       " ('something', 0.39856570959091187),\n",
       " ('Homie', 0.3948797285556793)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_total.wv.most_similar('Bart', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Similarity (similarity 사용)\n",
    "- 'Maggie'와 'baby'의 similarity 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44829723"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_total.wv.similarity('Maggie', 'baby')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
